[data]

train_data_dir = ../dnnseg_data/zerospeech/xitsonga/
val_data_dir = ../dnnseg_data/zerospeech/sample/
test_data_dir = ../dnnseg_data/zerospeech/xitsonga/

[settings]

outdir = ../results/dnnseg/conll20/xit
network_type = mle
task = segmenter
streaming = True
speaker_emb_dim = None
append_speaker_emb_to_inputs = False
append_speaker_emb_to_decoder_inputs = False
speaker_revnet_n_layers = None
use_gpu_if_available = False
label_map_file = xitsonga_sampa_to_ipa.csv
feature_map_file = xitsonga_sampa_to_feats.csv
plot_position_anchor = input

segtype = vad
filter_type = cochleagram
n_coef = 50
order = 0
residual_targets = False
data_normalization = norm_and_standardize
reduction_axis = freq

pad_seqs = True
mask_padding = False
min_len = 1000
max_len = 1000
curriculum_steps = 100
predict_backward = False
predict_forward = False
round_loss_weights = False
xent_state_predictions = True
use_mae = False
l2_normalize_targets = False
lm_order_bwd = 0
lm_order_fwd = 10
lm_loss_type = masked_neighbors
lm_masking_mode = drop_masked
lm_use_upper = True
lm_decode_from_encoder_states = False
lm_boundaries_as_attn = False
dtw_gamma = None
oracle_boundaries = None

n_iter = 50
n_pretrain_steps = 0
optim_name = Adam
n_samples = 1
weight_update_mode = all
use_jtps = False
max_global_gradient_norm = 1
epsilon = 1e-8
optim_epsilon = 1e-8
learning_rate = 1e-3
ema_decay = None
minibatch_size = 8
eval_minibatch_size = 256
save_freq = 0
log_freq = 0
eval_freq = 0

speaker_adversarial_gradient_scale = None
n_passthru_neurons = 0
passthru_adversarial_gradient_scale = None

loss_weights_gradient_scale = None

lm_loss_scale = 1
lm_target_gradient_scale = None
loss_normalization = layer

correspondence_loss_scale = None
correspondence_target_gradient_scale = None
n_units_correspondence_decoder = 128
sequential_cae = True
add_bottomup_correspondence_loss = False
correspondence_decoder_activation_inner = elu
n_units_speaker_decoder = 128 128
speaker_decoder_activation_inner = tanh
n_units_passthru_decoder = 128 128
passthru_decoder_activation_inner = tanh

encoder_type = HMLSTM
encoder_bptt = True
encoder_conv_kernel_size = 3
input_projection_activation_inner = tanh
input_projection_activation = tanh
n_units_input_projection = None
n_units_pre_cnn = None
n_units_pre_rnn = None
n_units_encoder = 128 128 128
;n_features_encoder = 32 32 None
encoder_resnet_n_layers_inner = None
hmlstm_kernel_depth = 1
hmlstm_prefinal_mode = max
encoder_batch_normalization_decay = None
batch_normalize_encodings = False
encoder_activation = tanh
encoder_inner_activation = tanh
encoder_prefinal_activation = elu
encoder_revnet_activation = tanh
encoder_recurrent_activation = sigmoid
encoder_boundary_activation =  sigmoid
encoder_dropout = None
encoder_boundary_implementation = 1
neurons_per_boundary = 1
boundary_neuron_agg_fn = max
neurons_per_feature = 1
feature_neuron_agg_fn = max
cumulative_boundary_prob = False
cumulative_feature_prob = False
forget_at_boundary = True
recurrent_at_forget = True
encoder_renormalize_preactivations = False
encoder_append_previous_features = True
encoder_append_seg_len = True
nested_boundaries = True
encoder_use_timing_unit = False
encoder_boundary_discretizer = bsn
encoder_state_discretizer = None
encoder_discretize_state_at_boundary = False
encoder_discretize_final = False
sample_at_train = True
sample_at_eval = False
temporal_dropout_plug_lm = False
temporal_dropout_rate = None
encoder_state_noise_level = None
encoder_feature_noise_level = None
encoder_boundary_noise_level = None
encoder_input_noise_level = None
encoder_bottomup_noise_level = None
encoder_use_bias = True
encoder_force_vad_boundaries = True
encoder_revnet_n_layers = None
encoder_l2_normalize_states = False
scale_losses_by_boundaries = False
min_discretization_prob = None
trainable_self_discretization = False
slope_annealing_rate = None
slope_annealing_max = 10
encoder_use_outer_product_memory = False

;decoder_type = CNN
;n_layers_decoder = 2
;n_units_decoder = 128
;decoder_inner_activation = elu
;decoder_conv_kernel_size = 7

;decoder_type = RNN
;n_layers_decoder = 1
;n_units_decoder = 128
;decoder_inner_activation = tanh

decoder_type = Seq2SeqAttn
decoder_seq2seq_implementation = 1
decoder_inner_activation = tanh
decoder_n_query_units = 32
decoder_project_attn_keys = True
decoder_use_gold_attn_keys_at_train = False
decoder_use_gold_attn_keys_at_eval = False
decoder_discretize_attn_keys = False
decoder_positional_attn_keys = False
decoder_gaussian_attn = True
backprop_into_attn_keys = True
decoder_discretize_refeed = False
backprop_into_refeed = False
decoder_encode_keys = False
decoder_add_positional_encoding = True
decoder_initialize_state_from_above = False
decoder_l2_normalize_states = False
decoder_append_seglen = False

decoder_hidden_state_expansion_type = tile
decoder_positional_encoding_lock_to_data = False
decoder_positional_encoding_type = transformer
decoder_positional_encoding_units = 128
decoder_activation = None
n_units_decoder_input_projection = 128
decoder_input_projection_activation_inner = tanh
decoder_input_projection_activation = tanh

segment_at_peaks = False
boundary_prob_smoothing = None
boundary_prob_discretization_threshold = None

encoder_state_regularization = None
;encoder_weight_regularization = l1_regularizer_0.1
encoder_weight_regularization = None
;encoder_feature_regularization = l1_regularizer_0.01
encoder_feature_regularization = None
;encoder_bitwise_feature_regularization = l1_regularizer_0.01
;encoder_bitwise_feature_regularization = None
encoder_feature_similarity_regularizer_scale = None
encoder_feature_similarity_regularizer_shape = 1
;boundary_regularizer_scale = 0.01
boundary_rate_extremeness_regularizer_scale = None
boundary_rate_extremeness_regularizer_shape = 10
feature_rate_extremeness_regularizer_scale = None
feature_rate_extremeness_regularizer_shape = 10
encoder_seglen_regularizer_scale = None
encoder_seglen_regularizer_shape = 1

